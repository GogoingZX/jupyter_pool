{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setting\" data-toc-modified-id=\"Setting-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting</a></span></li><li><span><a href=\"#Function\" data-toc-modified-id=\"Function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Function</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T16:20:05.098738Z",
     "start_time": "2021-11-28T16:20:05.092980Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "sys.path.append(\"/Users/xuzhu/Desktop/code/assistants\") # my package\n",
    "from toolbox.os_assistant import scan_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T16:20:06.125477Z",
     "start_time": "2021-11-28T16:20:06.123144Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/Users/xuzhu/Desktop/data/open_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T16:36:54.314137Z",
     "start_time": "2021-11-28T16:36:54.301439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_content</th>\n",
       "      <th>insult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my dog has flea problems help please</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maybe not take hime to dog park stupid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my dalmation is so cute I love him</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop posting stupid worthless garbage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mr licks ate my steak how to stop him</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quit buying worthless dog food stupid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          posting_content  insult\n",
       "0    my dog has flea problems help please       0\n",
       "1  maybe not take hime to dog park stupid       1\n",
       "2      my dalmation is so cute I love him       0\n",
       "3   stop posting stupid worthless garbage       1\n",
       "4   mr licks ate my steak how to stop him       0\n",
       "5   quit buying worthless dog food stupid       1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posting_data__temp = [\n",
    "    \"my dog has flea problems help please\",\n",
    "    \"maybe not take hime to dog park stupid\",\n",
    "    \"my dalmation is so cute I love him\",\n",
    "    \"stop posting stupid worthless garbage\",\n",
    "    \"mr licks ate my steak how to stop him\",\n",
    "    \"quit buying worthless dog food stupid\"\n",
    "]\n",
    "\n",
    "label__temp = [0, 1, 0, 1, 0, 1]\n",
    "posting_data__dict = {\"posting_content\": posting_data__temp}\n",
    "posting_data__df = pd.DataFrame(posting_data__dict)\n",
    "posting_data__df[\"insult\"] = label__temp\n",
    "posting_data__df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T17:11:28.052156Z",
     "start_time": "2021-11-28T17:11:28.040992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['my dog has flea problems help please', 0], dtype=object),\n",
       " array(['maybe not take hime to dog park stupid', 1], dtype=object),\n",
       " array(['my dalmation is so cute I love him', 0], dtype=object),\n",
       " array(['stop posting stupid worthless garbage', 1], dtype=object),\n",
       " array(['mr licks ate my steak how to stop him', 0], dtype=object),\n",
       " array(['quit buying worthless dog food stupid', 1], dtype=object)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['my dog has flea problems help please'],\n",
       " ['maybe not take hime to dog park stupid'],\n",
       " ['my dalmation is so cute I love him'],\n",
       " ['stop posting stupid worthless garbage'],\n",
       " ['mr licks ate my steak how to stop him'],\n",
       " ['quit buying worthless dog food stupid']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['my dog has flea problems help please',\n",
       " 'maybe not take hime to dog park stupid',\n",
       " 'my dalmation is so cute I love him',\n",
       " 'stop posting stupid worthless garbage',\n",
       " 'mr licks ate my steak how to stop him',\n",
       " 'quit buying worthless dog food stupid']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(posting_data__df.values)\n",
    "\n",
    "data__list = [list(i) for i in list(posting_data__df[[\"posting_content\"]].values)]\n",
    "data__list\n",
    "\n",
    "list(posting_data__df[\"posting_content\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T17:27:43.144429Z",
     "start_time": "2021-11-28T17:27:43.135628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'maybe'] 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_uniq_vocab_list(\n",
    "    df,\n",
    "    column\n",
    "):  \n",
    "    statements = list(df[column].values)\n",
    "    vocabulary_list = [s.strip().split(\" \") for s in statements] # [[w1, w2, w3], [w1, w2, w3],...]\n",
    "    \n",
    "    vocab_set = set()\n",
    "    for row in vocabulary_list:\n",
    "        vocab_set = vocab_set | set(row)\n",
    "            \n",
    "    uniq_vocab_list = list(vocab_set)\n",
    "    return uniq_vocab_list\n",
    "\n",
    "\n",
    "label_list = posting_data__df.iloc[:,-1].to_list()\n",
    "posting_vocab = create_uniq_vocab_list(\n",
    "    df=posting_data__df,\n",
    "    column=\"posting_content\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    posting_vocab[:2],\n",
    "    len(posting_vocab)\n",
    ")\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T17:37:52.102619Z",
     "start_time": "2021-11-28T17:37:52.086723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'b' is not in my vocabulary list\n",
      "The word 'a' is not in my vocabulary list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['word_vector', 'new_words'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['b', 'a']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_word_vector(\n",
    "    uniq_vocab_pool,\n",
    "    input_words # uniq set\n",
    "    \n",
    "):\n",
    "    results = {}\n",
    "    results[\"word_vector\"] = [0] * len(uniq_vocab_pool)\n",
    "    results[\"new_words\"] = []\n",
    "    \n",
    "    if input_words is set:\n",
    "        pass\n",
    "    else:\n",
    "        input_words = set(input_words)\n",
    "        \n",
    "    for word in input_words:\n",
    "        if word in uniq_vocab_pool:\n",
    "            matched_index = uniq_vocab_pool.index(word)\n",
    "            results[\"word_vector\"][matched_index] = 1\n",
    "        else:\n",
    "            print(\"The word '{0}' is not in my vocabulary list\".format(word))\n",
    "            results[\"new_words\"].append(word)\n",
    "            \n",
    "    return results\n",
    "        \n",
    "\n",
    "input__test = set([\"a\", \"stupid\", \"dog\", \"problems\", \"please\", \"b\"])\n",
    "\n",
    "match_results = create_word_vector(\n",
    "    uniq_vocab_pool=posting_vocab,\n",
    "    input_words=input__test\n",
    ")\n",
    "\n",
    "\n",
    "match_results.keys()\n",
    "input__word_vector = match_results[\"word_vector\"]\n",
    "input__new_words = match_results[\"new_words\"]\n",
    "\n",
    "input__word_vector[:15]\n",
    "input__new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T17:35:54.155374Z",
     "start_time": "2021-11-28T17:35:54.146969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_content</th>\n",
       "      <th>insult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my dog has flea problems help please</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maybe not take hime to dog park stupid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my dalmation is so cute I love him</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stop posting stupid worthless garbage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mr licks ate my steak how to stop him</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quit buying worthless dog food stupid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          posting_content  insult\n",
       "0    my dog has flea problems help please       0\n",
       "1  maybe not take hime to dog park stupid       1\n",
       "2      my dalmation is so cute I love him       0\n",
       "3   stop posting stupid worthless garbage       1\n",
       "4   mr licks ate my steak how to stop him       0\n",
       "5   quit buying worthless dog food stupid       1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posting_data__df\n",
    "\n",
    "positing_data__label = posting_data__df.iloc[:,-1].to_list()\n",
    "positing_data__uniq_vocab_pool = create_uniq_vocab_list(df=posting_data__df, column=\"posting_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T18:25:44.679890Z",
     "start_time": "2021-11-28T18:25:44.676579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'maybe', 'flea', 'ate', 'mr', 'help', 'love', 'has', 'take', 'buying', 'my', 'worthless', 'stop', 'garbage', 'problems', 'not', 'please', 'so', 'posting', 'food', 'steak', 'park', 'licks', 'cute', 'him', 'hime', 'dalmation', 'stupid', 'how', 'quit', 'is', 'dog', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(positing_data__uniq_vocab_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T17:42:56.596601Z",
     "start_time": "2021-11-28T17:42:56.575949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  label\n",
       "0  [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, ...      0\n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...      1\n",
       "2  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...      0\n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...      1\n",
       "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...      0\n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...      1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data__matrix = []\n",
    "\n",
    "for index, row in posting_data__df.iterrows():\n",
    "    word_list = row[\"posting_content\"].strip().split(\" \")\n",
    "    matched_results = create_word_vector(\n",
    "        uniq_vocab_pool=positing_data__uniq_vocab_pool,\n",
    "        input_words=word_list\n",
    "    )\n",
    "    training_data__matrix.append(matched_results[\"word_vector\"])\n",
    "\n",
    "\n",
    "# [word_vector, word_vector,...]\n",
    "type(training_data__matrix[0])\n",
    "\n",
    "training_data__dict = {\n",
    "    \"vector\": training_data__matrix,\n",
    "    \"label\": positing_data__label\n",
    "}\n",
    "training_data__df = pd.DataFrame(data=training_data__dict)\n",
    "training_data__df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T18:34:33.619182Z",
     "start_time": "2021-11-28T18:34:33.612413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.        , 0.33333333, 0.        , 0.33333333])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(positing_data__label)\n",
    "\n",
    "(np.zeros(5) + [1, 0, 1, 0, 1]) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({c_i}|\\vec{w}) = \\frac{p(\\vec{w} c_i)}{p(\\vec{w})}=\\frac{p(\\vec{w}|c_i) \\cdot p(c_i)}{p(\\vec{w})}$$\n",
    "\n",
    "Assume: $p(\\vec{w}|c_i) = p(w_0, w_1, \\cdots | c_i) = p(w_0|c_i) \\cdot p(w_1|c_i) \\cdot ...$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T18:34:34.745678Z",
     "start_time": "2021-11-28T18:34:34.739127Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_naive_bayesian(\n",
    "    training_data__matrix,\n",
    "    training_data__label\n",
    "):\n",
    "    \"\"\"\n",
    "    Here is the description\n",
    "    \"\"\"\n",
    "    \n",
    "    statement_qty = len(training_data__label)\n",
    "    uniq_vocab_qty = len(training_data__matrix[0]) # uniq vocabulary qty\n",
    "    prob__label_1 = sum(training_data__label) / statement_qty # p(c), here label = (0, 1) only 2 values\n",
    "    \n",
    "    label_0__vocab_array, label_1__vocab_array = np.zeros(uniq_vocab_qty), np.zeros(uniq_vocab_qty)\n",
    "    label_0__vocab_sum, label_1__vocab_sum = 0, 0\n",
    "    \n",
    "    for i in range(statement_qty):\n",
    "        if training_data__label[i] == 1: # label 1 statement\n",
    "            label_1__vocab_array += training_data__matrix[i]\n",
    "            label_1__vocab_sum += sum(training_data__matrix[i])\n",
    "        else: # label 0 statement\n",
    "            label_0__vocab_array += training_data__matrix[i]\n",
    "            label_0__vocab_sum += sum(training_data__matrix[i])\n",
    "    \n",
    "    label_1__vector = label_1__vocab_array / label_1__vocab_sum\n",
    "    label_0__vector = label_0__vocab_array / label_0__vocab_sum\n",
    "            \n",
    "    return label_0__vector, label_1__vector, prob__label_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T18:35:14.337757Z",
     "start_time": "2021-11-28T18:35:14.332775Z"
    }
   },
   "outputs": [],
   "source": [
    "p0v, p1v, p1b = train_naive_bayesian(training_data__matrix, positing_data__label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T18:35:29.570635Z",
     "start_time": "2021-11-28T18:35:29.564474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04166667, 0.        , 0.04166667, 0.04166667, 0.04166667,\n",
       "        0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "        0.125     , 0.        , 0.04166667, 0.        , 0.04166667,\n",
       "        0.        , 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "        0.04166667, 0.        , 0.04166667, 0.04166667, 0.08333333,\n",
       "        0.        , 0.04166667, 0.        , 0.04166667, 0.        ,\n",
       "        0.04166667, 0.04166667, 0.04166667]),\n",
       " array([0.        , 0.05263158, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "        0.        , 0.10526316, 0.05263158, 0.05263158, 0.        ,\n",
       "        0.05263158, 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "        0.        , 0.05263158, 0.        , 0.        , 0.        ,\n",
       "        0.05263158, 0.        , 0.15789474, 0.        , 0.05263158,\n",
       "        0.        , 0.10526316, 0.05263158]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v, p1v, p1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
